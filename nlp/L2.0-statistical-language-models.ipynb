{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FtsHjbEwUUP"
      },
      "source": [
        "##### Master Degree in Computer Science and Data Science for Economics\n",
        "\n",
        "# Statistical Language Models\n",
        "\n",
        "### Alfio Ferrara"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f26R-PwrwUUQ"
      },
      "source": [
        "Given a corpus $D$ of size $N$, we can compute the probability of a token $w \\in D$ as:\n",
        "\n",
        "$$\n",
        "P(w) = \\frac{count(w)}{\\sum\\limits_{i=0}^{N} count(w_i)}\n",
        "$$\n",
        "\n",
        "Using this simple statistics, we can sample a word from the corpus according to its probability $P(w)$. If we use this to generate a text $d = w_1, w_2, \\dots, w_{n-1}, w_n$ we have:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_{n-1}, w_n) = P(w_1)P(w_2) \\dots P(w_{n-1})P(w_n) = \\prod\\limits_{i=1}^{n} P(w_i)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ZLJqsZwUUQ"
      },
      "source": [
        "Howewer, this is not realistic. A better way to model this process is to choose words by taking into account the words we generated before, by sampling the $i$th word with a probabiity that is conditioned by the previous $i-1$ words.\n",
        "\n",
        "Thus, applying the chain rule:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_{n-1}, w_n) = P(w_1) P(w_2 \\mid w_1) \\dots P(w_{n-1} \\mid w_1 \\dots w_{n-2}) P(w_n \\mid w_1 \\dots w_{n-1}) = \\prod\\limits_{i=1}^{n} P(w_i \\mid w_1 \\dots w_{i-1})\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Xnw3uTIwUUS"
      },
      "source": [
        "Now, indexing long sequences as required by this method is unfeasible for a couple of good reasons:\n",
        "1. it's memory consuming and computationally intractable\n",
        "2. The logest the sequences the fewer the chances to observe such sequences a sufficient number of times\n",
        "\n",
        "So, we can apply a Markov approximation by taking into account only subsequences of lenght $k$:\n",
        "\n",
        "$$\n",
        "P(w_1, w_2, \\dots, w_{n-1}, w_n) = \\prod\\limits_{i=1}^{n} P(w_i \\mid w_{i-k} \\dots w_{i-1})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Se010kjtwUUS"
      },
      "outputs": [],
      "source": [
        "from nlp.langmodel import MarkovLM\n",
        "import pymongo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9VBydgMwUUS"
      },
      "source": [
        "## Get a corpus of texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAeADfLOwUUT"
      },
      "outputs": [],
      "source": [
        "db = pymongo.MongoClient()['cousine']\n",
        "recipes = db['foodcom']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELxLmIn6wUUT"
      },
      "outputs": [],
      "source": [
        "def create_corpus(query:  dict = {}, numdocs: int = 3000):\n",
        "    corpus = []\n",
        "    for recipe in recipes.find(query).limit(numdocs):\n",
        "        for sentence in recipe['steps']:\n",
        "            corpus.append(sentence)\n",
        "    return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAHqMOhFwUUU",
        "outputId": "cec83871-8df0-4339-d3de-994a0271bb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Corpus size: 20262\n",
            "I a sauce pan, bring water to a boil; slowly add grits and salt, stirring constantly; Reduce heat:simmer, uncovered, for 40-45 minutes or untill thickened, stirrin occasionally.\n",
            "Add cheese and garlic; stir until cheese is melted, Spray 9-inch baking dish with nonstick cooking spray; Cover and refrigerate for 2 to 2 1/2 hours or until frim.\n",
            "Before starting the grill, coat the grill rack with nonstick cooking spray; Cut the grits into 3-inch squares; Brush both sides with olive oil.\n",
            "Grill, covered, over medium heat for 4 to 6 minutes on each side or until lightly browned.\n"
          ]
        }
      ],
      "source": [
        "numdocs = 3000\n",
        "corpus = create_corpus(query={}, numdocs=numdocs)\n",
        "print(f\"Corpus size: {len(corpus)}\")\n",
        "for text in corpus[:4]:\n",
        "    print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-order markov assumption (n-gram Markov Language Model)\n",
        "We assume that a word (to predict) depends only on the previous n words.\n",
        "$$P(W_1...W_k) = \\prod_{i=1}^{k}  P(W_k | W_1,..., W_k-1)  $$\n",
        "\n",
        "* k=2:\n",
        "specifies a 2nd-order Markov model, also known as a bigram model.\n",
        "This model predicts the probability of the next word based on the single preceding word.\n",
        "The model learns the probabilities of sequences of two words (bigrams).\n",
        "\n",
        "* k=4: specifies a 4th-order Markov model, also known as a four-gram model.\n",
        "This model predicts the probability of the next word based on the three preceding words.\n",
        "The model learns the probabilities of sequences of four words (four-grams)."
      ],
      "metadata": {
        "id": "kdogp_2dItem"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd-rlQllwUUV"
      },
      "outputs": [],
      "source": [
        "tokenizer = \"bert-base-uncased\"\n",
        "brlm = MarkovLM(k=2, tokenizer_model=tokenizer) #Bigram-based Markov LM\n",
        "frlm = MarkovLM(k=4, tokenizer_model=tokenizer) #Four-gram-based Markov LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idan3QpcwUUV",
        "outputId": "e9d039cf-9f2e-4dce-89be-e78291bfb9a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20262/20262 [00:02<00:00, 8610.73it/s]\n",
            "100%|██████████| 20262/20262 [00:02<00:00, 7508.11it/s]\n"
          ]
        }
      ],
      "source": [
        "brlm.train(corpus=corpus)\n",
        "frlm.train(corpus=corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3rh5qvswUUW"
      },
      "source": [
        "## Text generation\n",
        " MarkovLM().generate() would use the trained n-gram Markov Language Model to produce a new sequence of text. This process is often called text generation or sampling.  \n",
        " MarkovLM().generate()  (or any similar text generation function) will likely not give the same result each time. It will produce a new, different sequence of text with each run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvwEuPrawUUW",
        "outputId": "962a9101-f18e-4eb8-9f02-baf3ed328d55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[#S]',\n",
              " 'pre',\n",
              " '##hea',\n",
              " '##t',\n",
              " 'slowly',\n",
              " 'add',\n",
              " 'the',\n",
              " 'cottage',\n",
              " 'cheese',\n",
              " 'and',\n",
              " 'then',\n",
              " 'add',\n",
              " 'the',\n",
              " 'oven',\n",
              " 'for',\n",
              " '30',\n",
              " 'minutes',\n",
              " '.',\n",
              " '[#E]']"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "brlm.generate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbYRwefqwUUW",
        "outputId": "e11dcec1-49c7-404d-b6e3-fdeeda7c6dcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2gram:  [#S] with walnuts in the beans and bring to cover and 1 minute . [#E]\n",
            "4gram:  [#S] [#S] [#S] in a heavy skillet , saute ham with onions and dot with butter . [#E]\n"
          ]
        }
      ],
      "source": [
        "print(\"2gram: \", \" \".join(brlm.generate()).replace(\" ##\", \"\"))\n",
        "print(\"4gram: \", \" \".join(frlm.generate()).replace(\" ##\", \"\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_P-zDqpwUUW"
      },
      "source": [
        "## Text classification\n",
        "The primary purpose of MarkovLM.log_prob() is to compute the log-probability of a given sequence of tokens occurring according to the model's learned transition probabilities.\n",
        "It's a method for calculating the probability of a given sequence of text under a trained Markov Language Model. While this probability score can be used for a form of text classification, it's not a direct or robust method.\n",
        "Note that the MarkovLM() should be trained separately for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8cUGpzhwUUW",
        "outputId": "1041e308-f4dd-45d8-b292-4f6d7d445b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Italian: 30052\n",
            "Chinese: 24134\n"
          ]
        }
      ],
      "source": [
        "italian_q = {'search_terms': 'italian'}\n",
        "chinese_q = {'search_terms': 'chinese'}\n",
        "numdocs = 3000\n",
        "italian_corpus = create_corpus(query=italian_q, numdocs=numdocs)\n",
        "chinese_corpus = create_corpus(query=chinese_q, numdocs=numdocs)\n",
        "print(f\"Italian: {len(italian_corpus)}\")\n",
        "print(f\"Chinese: {len(chinese_corpus)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLk6tF_IwUUW"
      },
      "outputs": [],
      "source": [
        "it = MarkovLM(k=4, tokenizer_model=tokenizer)\n",
        "ch = MarkovLM(k=4, tokenizer_model=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWsp3wdewUUW",
        "outputId": "be9cafcd-682d-4a04-9a2f-152f11e9cf5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30052/30052 [00:03<00:00, 7742.52it/s]\n",
            "100%|██████████| 24134/24134 [00:03<00:00, 6844.65it/s]\n"
          ]
        }
      ],
      "source": [
        "it.train(corpus=italian_corpus)\n",
        "ch.train(corpus=chinese_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zaXrbHd7wUUW",
        "outputId": "151ec38b-ad91-4e34-acfc-cf058a65e781"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Italian sentence: Place ravioli on a large baking sheet sprinkled with cornstarch.\n",
            "Italian: -23.0900057470743\n",
            "Chinese: -21.352223087931076\n",
            "========\n",
            "Chinese sentence: Heat enough oil in a frying pan over medium heat to shallow fry.\n",
            "Italian: -24.643378642963395\n",
            "Chinese: -22.686133859693925\n"
          ]
        }
      ],
      "source": [
        "italian_sentence = italian_corpus[6]\n",
        "chinese_sentence = chinese_corpus[6]\n",
        "\n",
        "print(f\"Italian sentence: {italian_sentence}\")\n",
        "print(f\"Italian: {it.log_prob(italian_sentence)}\")\n",
        "print(f\"Chinese: {ch.log_prob(italian_sentence)}\")\n",
        "print(\"========\")\n",
        "print(f\"Chinese sentence: {chinese_sentence}\")\n",
        "print(f\"Italian: {it.log_prob(chinese_sentence)}\")\n",
        "print(f\"Chinese: {ch.log_prob(chinese_sentence)}\")\n",
        "#a log-probability of -23.09 indicates a much less likely sequence than a log-probability of -21.35"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL3ViydzwUUY"
      },
      "source": [
        "## Combine languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Y5Aj_CuwUUY"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfNr0lRkwUUY"
      },
      "outputs": [],
      "source": [
        "mix = copy.deepcopy(it)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phfJm3e2wUUY",
        "outputId": "eba5dae7-9549-47d0-b05c-b5ccc3052258"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 24134/24134 [00:03<00:00, 7124.59it/s]\n"
          ]
        }
      ],
      "source": [
        "mix.train(chinese_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Deb8SC04wUUZ",
        "outputId": "5b94190a-6e85-4a35-822d-a1a25fe2c360"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mix:  [#S] [#S] [#S] add squash and sage ; saute in extra virgin olive oil again . close the lid , turn up the heat to medium . pour water into a small cigar shape . make sure rice takes up soya sauce and 1 / 4 teaspoon , scoop out any seeds and then press down on it slightly since eggplant . slice eggplant on a folded brown paper bag or on paper towels . [#E]\n"
          ]
        }
      ],
      "source": [
        "print(\"Mix: \", \" \".join(mix.generate()).replace(\" ##\", \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkkDXcIwwUUZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}