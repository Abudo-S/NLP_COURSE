{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "652XpmQ3R0sO"
      },
      "source": [
        "##### Master Degree in Computer Science and Data Science for Economics\n",
        "\n",
        "# Generation by RNN\n",
        "\n",
        "### Alfio Ferrara"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s1hmPatR0sQ",
        "outputId": "05e09657-5905-471c-8976-cbb2a13f3afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.4077\n",
            "Epoch 2/10, Loss: 1.0863\n",
            "Epoch 3/10, Loss: 1.0401\n",
            "Epoch 4/10, Loss: 1.0177\n",
            "Epoch 5/10, Loss: 1.0064\n",
            "Epoch 6/10, Loss: 1.0012\n",
            "Epoch 7/10, Loss: 0.9971\n",
            "Epoch 8/10, Loss: 0.9948\n",
            "Epoch 9/10, Loss: 0.9937\n",
            "Epoch 10/10, Loss: 0.9915\n",
            "Generazione di un numero romano:\n",
            "<START>MMMCCCXXXIII\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "roman_map = {\n",
        "    1: \"I\", 4: \"IV\", 5: \"V\", 9: \"IX\", 10: \"X\",\n",
        "    40: \"XL\", 50: \"L\", 90: \"XC\", 100: \"C\",\n",
        "    400: \"CD\", 500: \"D\", 900: \"CM\", 1000: \"M\"\n",
        "}\n",
        "\n",
        "def decimal_to_roman(num):\n",
        "    roman = \"\"\n",
        "    for value, symbol in sorted(roman_map.items(), key=lambda x: -x[0]):\n",
        "        while num >= value:\n",
        "            roman += symbol\n",
        "            num -= value\n",
        "    return roman\n",
        "\n",
        "def roman_to_decimal(roman):\n",
        "    roman_map = {\n",
        "        'I': 1, 'V': 5, 'X': 10, 'L': 50,\n",
        "        'C': 100, 'D': 500, 'M': 1000\n",
        "    }\n",
        "    total = 0\n",
        "    prev_value = 0\n",
        "\n",
        "    for char in reversed(roman):\n",
        "        current_value = roman_map[char]\n",
        "        if current_value >= prev_value:\n",
        "            total += current_value\n",
        "        else:\n",
        "            total -= current_value\n",
        "        prev_value = current_value\n",
        "\n",
        "    return total\n",
        "\n",
        "class RomanDataset(Dataset):\n",
        "    def __init__(self, max_number=3999):\n",
        "        self.data = [decimal_to_roman(i) for i in range(1, max_number + 1)]\n",
        "        self.char_to_idx = {char: idx + 1 for idx, char in enumerate(\"IVXLCDM\")}  # +1 for padding\n",
        "        self.char_to_idx[\"<START>\"] = len(self.char_to_idx) + 1 #start token\n",
        "        self.char_to_idx[\"<END>\"] = len(self.char_to_idx) + 1 #end token\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "        self.pad_idx = 0\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        roman = self.data[idx]\n",
        "        tokens = [self.char_to_idx[\"<START>\"]] + [self.char_to_idx[char] for char in roman] + [self.char_to_idx[\"<END>\"]]\n",
        "        return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "'''\n",
        "Takes a list of tensors with different lengths and pads them to the same length.\n",
        "This is an essential preprocessing step for creating batches of sequential data for RNNs\n",
        "'''\n",
        "def collate_fn(batch):\n",
        "    return pad_sequence(batch, batch_first=True, padding_value=0)\n",
        "\n",
        "# LSTM model\n",
        "class RomanGenerator(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
        "        super(RomanGenerator, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x) #The shape after embedding is (batch_size, sequence_length, embed_size)\n",
        "        output, hidden = self.lstm(x, hidden) #The shape of output (batch_size, sequence_length, hidden_size)\n",
        "        output = self.fc(output) #Map hidden_size to vocab_size (scores of next tokens)\n",
        "        return output, hidden\n",
        "\n",
        "# parameters\n",
        "max_number = 3999\n",
        "embed_size = 16 #The embedding size determines the dimensionality of the vector representation for each character in the vocabulary\n",
        "hidden_size = 128 #(hyperparameter) This size influences the model's capacity to learn and remember patterns in the sequence data. [128 is a common choice]\n",
        "num_layers = 1\n",
        "batch_size = 32\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# Dataset and DataLoader\n",
        "dataset = RomanDataset(max_number=max_number)\n",
        "vocab_size = len(dataset.char_to_idx) + 1  # +1 per il padding\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# Model, loss and optimization\n",
        "model = RomanGenerator(vocab_size, embed_size, hidden_size, num_layers)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch[:, :-1]  # Everything but the last token\n",
        "        targets = batch[:, 1:]  # Everything but the first token\n",
        "        outputs, _ = model(inputs)\n",
        "        #print(f'outputs{outputs.shape}, targets {targets.shape}')\n",
        "        loss = criterion(outputs.reshape(-1, vocab_size), targets.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(dataloader):.4f}\")\n",
        "\n",
        "# Generate roman numbers\n",
        "def generate_sequence(model, start_token, max_length, dataset):\n",
        "    model.eval()\n",
        "    generated_sequence = [start_token]\n",
        "    input_seq = torch.tensor([[start_token]], dtype=torch.long)\n",
        "    hidden = None\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_seq, hidden) #pass the previous hidden state\n",
        "            #select the token with the highest token score value from the model's output at the last time step\n",
        "            #Note that argmax can't be used within NN cause it outputs 1 for the max value and 0 for any other value, and since it's a constant function its derivate is zero (can't optimize backpropagation)\n",
        "            next_token = torch.argmax(output[:, -1, :], dim=-1).item()\n",
        "            if next_token == dataset.char_to_idx[\"<END>\"]:\n",
        "                break\n",
        "            generated_sequence.append(next_token)\n",
        "            input_seq = torch.tensor([[next_token]], dtype=torch.long)\n",
        "\n",
        "    return \"\".join([dataset.idx_to_char[idx] for idx in generated_sequence if idx > 0])\n",
        "\n",
        "# Example\n",
        "start_token = dataset.char_to_idx[\"<START>\"]\n",
        "print(\"Generazione di un numero romano:\")\n",
        "print(generate_sequence(model, start_token, max_length=20, dataset=dataset))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S9f-63LRR0sS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tempreature and multinominal() for randomness\n",
        "The temperature is a hyperparameter that controls the randomness of the sampling process.  \n",
        "Dividing by Temperature: Dividing the logits by the temperature value effectively scales these scores.  \n",
        "\n",
        "Effect on Probabilities (after Softmax):\n",
        "- High Temperature (e.g., > 1.0): Dividing by a temperature greater than 1 makes the logits smaller. When these smaller logits are passed through the softmax function, the resulting probability distribution becomes flatter. This means there is less difference between the probabilities of the most likely tokens and less likely tokens. Sampling from a flatter distribution is more random, leading to more diverse and potentially unexpected generated sequences.  \n",
        "- Low Temperature (e.g., < 1.0): Dividing by a temperature less than 1 makes the logits larger. When these larger logits are passed through the softmax function, the resulting probability distribution becomes sharper. This means the probabilities of the most likely tokens are increased, while the probabilities of less likely tokens are decreased. Sampling from a sharper distribution is less random, leading to generated sequences that are more deterministic and closer to the most probable output according to the model.\n",
        "- Temperature = 1.0: Dividing by 1.0 has no effect on the logits. The sampling is based directly on the probabilities from the standard softmax output. This is the default behavior if no temperature scaling is applied.\n",
        "\n"
      ],
      "metadata": {
        "id": "gGrNRbPyBzF0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UdKnASvuR0sT"
      },
      "outputs": [],
      "source": [
        "def read_sequence(model, start_tokens, max_length, dataset, temperature=1.0):\n",
        "    model.eval()\n",
        "    generated_sequence = start_tokens[:] #indicis\n",
        "    #print(generated_sequence)\n",
        "    input_seq = torch.tensor([start_tokens], dtype=torch.long)\n",
        "    with torch.no_grad():\n",
        "        _, hidden = model(input_seq)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        with torch.no_grad():\n",
        "            output, hidden = model(input_seq, hidden)\n",
        "            logits = output[:, -1, :]\n",
        "            probabilities = F.softmax(logits / temperature, dim=-1)\n",
        "            for i, x in enumerate(probabilities[0].numpy()):\n",
        "                if i > 0:\n",
        "                    print(dataset.idx_to_char[i], np.round(x, 2))\n",
        "\n",
        "            #torch.multinomial() allows us to sample the next character based on these probabilities.\n",
        "            #This introduces an element of randomness, making the generated sequences more varied and potentially more realistic\n",
        "            next_token = torch.multinomial(probabilities, num_samples=1).item()\n",
        "            print(f'next token idx: {next_token}')\n",
        "            if next_token == dataset.char_to_idx[\"<END>\"]:\n",
        "                break\n",
        "            generated_sequence.append(next_token)\n",
        "            input_seq = torch.tensor([[next_token]], dtype=torch.long)\n",
        "            print([dataset.idx_to_char[idx] for idx in generated_sequence if idx > 0])\n",
        "\n",
        "    return \"\".join([dataset.idx_to_char[idx] for idx in generated_sequence if idx > 0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6di0rXHnR0sT",
        "outputId": "ef62a417-8f30-4ac3-befe-7d6af347480c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I 0.0\n",
            "V 0.0\n",
            "X 0.0\n",
            "L 0.0\n",
            "C 0.0\n",
            "D 0.0\n",
            "M 0.0\n",
            "<START> 0.0\n",
            "<END> 1.0\n",
            "next token idx: 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<START>XII'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "start_tokens = start_tokens = [\n",
        "    dataset.char_to_idx[\"<START>\"],\n",
        "    dataset.char_to_idx[\"X\"],\n",
        "    dataset.char_to_idx[\"I\"],\n",
        "    dataset.char_to_idx[\"I\"],\n",
        "    ]\n",
        "read_sequence(model, start_tokens, max_length=10,\n",
        "            dataset=dataset, temperature=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7y6vO7j1R0sU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qv-po1boR0sU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}