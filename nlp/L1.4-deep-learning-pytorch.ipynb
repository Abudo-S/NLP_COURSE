{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cce4b1e2",
      "metadata": {
        "id": "cce4b1e2"
      },
      "source": [
        "##### Master Degree in Computer Science and Data Science for Economics\n",
        "\n",
        "# Primer about PyTorch for Deep Learning\n",
        "\n",
        "### Alfio Ferrara (materials from Darya Shlyk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73624033",
      "metadata": {
        "id": "73624033"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> PyTorch's computation graphs  </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7fe0b6e",
      "metadata": {
        "id": "d7fe0b6e"
      },
      "source": [
        "PyTorch performs its computations based on a **directed acyclic graph (DAG)**. \\\n",
        "The graph is called computation graph: nodes in the graph are Tensors, and edges are functions that produce output Tensors from input Tensors.\\\n",
        "PyTorch builds computation graph on the fly and uses it to derive the relationships between tensors and easily compute the gradients.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d42788a7",
      "metadata": {
        "id": "d42788a7"
      },
      "source": [
        "#### Computation graph implementing the equation ``y = log((w * x) + b)``"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a5c554",
      "metadata": {
        "id": "73a5c554"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> Computation graph and Neural Networks</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d24cdcf",
      "metadata": {
        "id": "2d24cdcf"
      },
      "source": [
        "Neural networks (NNs) are a collection of nested functions that transform the input data.\\\n",
        "These functions are defined by parameters (weights and biases), which in PyTorch are stored in tensors.\\\n",
        "In PyTorch special tensors are used to store and update the parameters of our model during training.\\\n",
        "To create such a tensor simply set ``requires_grad`` attribute to ``True``. By default a tensor is created with ``requires_grad = False``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2ee4c512",
      "metadata": {
        "id": "2ee4c512"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f55010c5",
      "metadata": {
        "id": "f55010c5",
        "outputId": "eea0c213-433d-475b-e344-26fb8f277cb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5.1897e+30, 4.4727e-41, 5.1900e+30, 4.4727e-41, 5.1902e+30, 4.4727e-41],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "w, b = torch.Tensor(3), torch.Tensor(6)\n",
        "w.requires_grad_()        # w = torch.rand(1, requires_grad = True)\n",
        "b.requires_grad_()        # b = torch.ones(1, requires_grad = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a088d45",
      "metadata": {
        "id": "8a088d45"
      },
      "source": [
        "Using a computation graph PyTorch decomposes the network computation into a set of simpler functions.\\\n",
        "Computational graphs is at the core of the Neural Network training, which happens in two steps:\n",
        "* a forward pass: the NN runs the input data through each of its functions to make the best guess about the correct output.\n",
        "* a backward pass: propagate the error signal through the network, by means of gradients and adjust the model parameters accordingly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a656d8b3",
      "metadata": {
        "id": "a656d8b3"
      },
      "source": [
        "![photo_2023-01-09_17-37-42.jpg](attachment:photo_2023-01-09_17-37-42.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ec8aec3",
      "metadata": {
        "id": "6ec8aec3"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> Automatic differentiation </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d18fc0b",
      "metadata": {
        "id": "5d18fc0b"
      },
      "source": [
        "Optimizing NNs requires computing the gradients of the loss with respect to the NN weights. \\\n",
        "PyTorch supports *automatic differentiation*, which can be thought of as an implementation of the *chain rule* for computing gradients of nested functions.\\\n",
        "Using PyTorch’s automatic differentiation, the forward pass of your network will define a computational graph.\\\n",
        "After the forward pass, the graph can be traversed backwards to calculate the gradients starting from the result of the forward evaluation.\\\n",
        "By calling ``backward`` method, we can compute the gradients of a tensor with respect to its dependent leaf nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "214a85eb",
      "metadata": {
        "id": "214a85eb",
        "outputId": "b02e958c-a33c-4b74-f2e4-bf8c7141dd3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dy/dw: tensor(0.2000)\n",
            "dy/db tensor(0.2000)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# create tensors\n",
        "x = torch.tensor(1.)\n",
        "w = torch.tensor(2., requires_grad=True)\n",
        "b = torch.tensor(3., requires_grad=True)\n",
        "\n",
        "# Build computation graph\n",
        "y = torch.log((x * w) + b)\n",
        "\n",
        "# Compute gradients\n",
        "y.backward()\n",
        "\n",
        "# Print gradients\n",
        "print(\"dy/dw:\", w.grad)\n",
        "print(\"dy/db\",  b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2114c5ab",
      "metadata": {
        "id": "2114c5ab"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b>  Supervised gradient-based learning  </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17534f79",
      "metadata": {
        "id": "17534f79"
      },
      "source": [
        "\n",
        "To perform a supervised learning we need 3 components (apart form the training data, i.e. input-target pairs):\n",
        "* a **model** that computes predictions from the observed input features\n",
        "* a **loss function** that informs how far off the model's guess is from the correct answer (the target)\n",
        "* an **optimizer** that makes the appropriate adjustment to the model's parameters, trying to align its predictions to the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0727220",
      "metadata": {
        "id": "b0727220"
      },
      "source": [
        "The learning starts with mistakes. We use an appropriate loss function to tell us how far off the model's predictions are from the correct answer. \\\n",
        "Having computed the prediction error, we propagate it through the network **[Backpropagation]**. To do this, we compute the gradient of the loss function with respect to the parameters of the model. The gradient signals *\"how much\"* the parameters should change in order to reduce the loss at the next guess.\\\n",
        "Backward propagation is kicked off when we call ``.backward()`` on the ``error`` tensor. PyTorch **AutoGrad** computes the gradients for each model parameter and stores them in the ``.grad`` attribute of the tensor. We use an optimization algorithm ( optimizer ) to update the model parameters using these gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cdeeebf",
      "metadata": {
        "id": "2cdeeebf"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> Building single layer NN model from scratch</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d07d12e4",
      "metadata": {
        "id": "d07d12e4"
      },
      "source": [
        "Suppose we need to build a model for binary classification.\\\n",
        "We create a toy dataset of 5 examples and 2 features: ``inputs``.\\\n",
        "Our goal is to assign a binary class (0 or 1) to each example based on its features.\\\n",
        "``targets`` provide the correct class we would like our model to predict for each example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e628ad5e",
      "metadata": {
        "id": "e628ad5e",
        "outputId": "8ba8335f-4cb1-4e62-ac1c-ef28f7f8161d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.1526,  0.8668],\n",
            "        [-1.3665, -0.5493],\n",
            "        [ 1.0053, -0.4016],\n",
            "        [ 0.1485,  0.2838],\n",
            "        [-0.0609, -1.0048]])\n",
            "tensor([1., 1., 1., 0., 0.])\n"
          ]
        }
      ],
      "source": [
        "inputs = torch.randn(5, 2)                                      # 5 examples with 2 features each\n",
        "targets = torch.tensor([1, 1, 1, 0, 0], dtype=torch.float32)    # a binary class for each of 5 examples\n",
        "print(inputs) #data examples\n",
        "print(targets) #labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "351b87c2",
      "metadata": {
        "id": "351b87c2"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> Model </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25510db4",
      "metadata": {
        "id": "25510db4"
      },
      "source": [
        "We need to define a model for our task. \\\n",
        "A model is a function that determines how to transform the inputs to generate the outputs.\\\n",
        "Here, we define our model as ``y = sigmoid( inputs @ weight + bias )``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f0e1c96a",
      "metadata": {
        "id": "f0e1c96a"
      },
      "outputs": [],
      "source": [
        "def model(inputs, weight, bias):\n",
        "    logits = inputs @ weight + bias         # linear layer\n",
        "    activations = torch.sigmoid(logits)     # activation layer\n",
        "    return activations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3e7dab5",
      "metadata": {
        "id": "e3e7dab5"
      },
      "source": [
        "We initialize the parameters of our model, ``weight`` and ``bias``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "79dd0a2c",
      "metadata": {
        "id": "79dd0a2c",
        "outputId": "224468f8-2c88-4ba2-f6e7-cd68991b4c05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0981, 0.0216], requires_grad=True)\n",
            "tensor([1.], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "weight = torch.randn(inputs.size(-1), requires_grad=True) #weight vector of feature dimenstionality\n",
        "bias   = torch.ones(1, requires_grad=True)\n",
        "print(weight)\n",
        "print(bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa5afc9c",
      "metadata": {
        "id": "fa5afc9c"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> Loss function </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "666643c7",
      "metadata": {
        "id": "666643c7"
      },
      "source": [
        "After defining the model, we can define the loss function that we want to minimize to find the optimal model weights.\\\n",
        "We will use **Binary Cross-Entropy loss** function, which is a natural choice for binary classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92c0780c",
      "metadata": {
        "id": "92c0780c"
      },
      "source": [
        "``y_pred`` is a vector containting the probability of being class 1 for each training example.\\\n",
        "``y_true`` is a vector of correct labels made of 0's and 1's."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "159197ee",
      "metadata": {
        "id": "159197ee"
      },
      "outputs": [],
      "source": [
        "def BinaryCrossEntropyLoss(y_pred, y_true):\n",
        "    return (-y_true * torch.log(y_pred) - (1-y_true) * torch.log(1-y_pred)).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a82d157",
      "metadata": {
        "id": "9a82d157"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> Breaking down training via SGD </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a425999",
      "metadata": {
        "id": "5a425999"
      },
      "source": [
        "To learn the weight parameters of the model, we will use the **stochastic gradient descent** algorithm.\\\n",
        "We implement the training procedure step-by-step, to show what the training algorithm looks like.\\\n",
        "Here, we manually update the model parameters with computed gradients, but we will never do it again in practice! \\\n",
        "PyTorch's ``torch.optim`` package provides loads of optimization algorithms that will be happy to perform the parameter adjustment for you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a792efc",
      "metadata": {
        "id": "5a792efc"
      },
      "source": [
        "#### Step 1. Call the model on inputs [Forward pass]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "32325c17",
      "metadata": {
        "id": "32325c17",
        "outputId": "88c67ba2-654f-4ab6-884b-049dca5a35af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7318, 0.7014, 0.7484, 0.7351, 0.7256], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "predictions = model(inputs, weight=weight, bias=bias)\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "961c6d98",
      "metadata": {
        "id": "961c6d98"
      },
      "source": [
        "#### Step 2. Evaluate the prediction error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "80f9fe96",
      "metadata": {
        "id": "80f9fe96",
        "outputId": "c21975de-88d9-43ae-cf82-2d419b144775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7156572341918945"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "error = BinaryCrossEntropyLoss(predictions, targets)\n",
        "error.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af825180",
      "metadata": {
        "id": "af825180"
      },
      "source": [
        "#### Step 3. Compute the gradients [Backprop]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f31a6f47",
      "metadata": {
        "id": "f31a6f47",
        "outputId": "0bbafdad-377e-4ed5-d9f8-65f1278224cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 0.0522, -0.0976]), tensor([0.1285]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "error.backward()\n",
        "weight.grad, bias.grad     # compute gradients for linear layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "907d7e87",
      "metadata": {
        "id": "907d7e87"
      },
      "source": [
        "#### Step 4. Update parameters using gradients\n",
        "Note that if we perform manual weight and bias tensors' update, we should't call loss.backward() to calculate them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1abda6d3",
      "metadata": {
        "id": "1abda6d3"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "#no_grad() ensures that manual update operations are not tracked by PyTorch's autograd engine (the computational graph won't trace the performed updates on tensors)\n",
        "with torch.no_grad(): #freeze gradient update to perform manual update of weight and bias\n",
        "    weight -= weight.grad * learning_rate\n",
        "    bias -= bias.grad * learning_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b96ff4e",
      "metadata": {
        "id": "9b96ff4e"
      },
      "source": [
        "#### Step 5. Reset the gradients to zero **[Important]**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7354e4a",
      "metadata": {
        "id": "a7354e4a"
      },
      "source": [
        "Having updated the parameters of the model,\\\n",
        "we zero-out the current gradients to compute the new ones in the next training step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9c88375e",
      "metadata": {
        "id": "9c88375e",
        "outputId": "fb6d69ac-8047-4162-da66-f97023f385c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#necessary to clear the previously computed gradients stored inside the model even after loss.backward()\n",
        "weight.grad.zero_()\n",
        "bias.grad.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7073cf68",
      "metadata": {
        "id": "7073cf68"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b>Building NN using torch.nn and torch.optim modules </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f07b2a",
      "metadata": {
        "id": "15f07b2a"
      },
      "source": [
        "``torch.nn`` is a PyTorch Neural Network module, that makes it easy to build and train NNs.\\\n",
        "It provides predefined layers that you can use as LEGO blocks to compose your NN models. \\\n",
        "Many layers inside a neural network are parameterized, i.e. have associated weights and biases that are optimized during training.\\\n",
        "Subclassing ``torch.nn.Module`` automatically tracks all fields defined inside your model object,\\\n",
        "and makes all parameters accessible using your model’s `parameters()` or `named_parameters()` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "feb56c9b",
      "metadata": {
        "id": "feb56c9b"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    \"\"\"Single-layer neural network with a sigmoid activation.\"\"\"\n",
        "\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # define layers of your Neural Network\n",
        "        self.linear  = nn.Linear(in_features = num_features, out_features = 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        logits = self.linear(inputs)            # linear layer\n",
        "        activations = self.sigmoid(logits)      # activation layer\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3c47dc27",
      "metadata": {
        "id": "3c47dc27"
      },
      "outputs": [],
      "source": [
        "# instantiate the model\n",
        "num_features = inputs.size(-1)\n",
        "model = NeuralNet(num_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b7c8517c",
      "metadata": {
        "id": "b7c8517c",
        "outputId": "cc2f23fb-58fa-45e1-8c2a-6a4facd8fa1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model architecture: NeuralNet(\n",
            "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"Model architecture:\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a8fd201e",
      "metadata": {
        "id": "a8fd201e",
        "outputId": "809e777f-0f0a-4470-9e27-494333803b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "linear.weight torch.Size([1, 2])\n",
            "linear.bias torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "for param_name, tensor in model.named_parameters():\n",
        "    print(param_name, tensor.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cfc2d02",
      "metadata": {
        "id": "3cfc2d02"
      },
      "source": [
        "#### Forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4c52be06",
      "metadata": {
        "id": "4c52be06"
      },
      "outputs": [],
      "source": [
        "predictions = model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea326cd",
      "metadata": {
        "id": "9ea326cd"
      },
      "source": [
        "#### Using a loss function from torch.nn module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "015254ec",
      "metadata": {
        "id": "015254ec"
      },
      "outputs": [],
      "source": [
        "# instantiate a binary cross-entropy loss\n",
        "loss_function = nn.BCELoss()\n",
        "\n",
        "# compute the error\n",
        "error = loss_function(predictions.squeeze(), targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e10a2bc1",
      "metadata": {
        "id": "e10a2bc1"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> Optimization algorithm </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e9f88f6",
      "metadata": {
        "id": "0e9f88f6"
      },
      "source": [
        "Choose an optimizer and register all the parameters of the model in the optimizer.\\\n",
        "Here we are using Stochastic Gradient Descent (SGD) with a learning rate of 0.01.\\\n",
        "The learning rate is a hyperparameter that controls how much impact the error signal has on updating the model weights.\\\n",
        "Large learning rate will cause bigger changes and may lead to overshoot the optimal parameter configuration,\\\n",
        "while small learning rate will result in slow training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a5e3c604",
      "metadata": {
        "id": "a5e3c604"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "# update model parameters\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6107a6db",
      "metadata": {
        "id": "6107a6db"
      },
      "source": [
        "## <h3 style=\"text-align: center;\"><b> Input pipeline </b>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd552e07",
      "metadata": {
        "id": "fd552e07"
      },
      "source": [
        "PyTorch provides two data loading utilities: ``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\\\n",
        "``Dataset`` stores the data and provides access to labeled pairs by indexing.\\\n",
        "``DataLoader`` groups and collates data points stored in the ``Dataset``."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "47f26142",
      "metadata": {
        "id": "47f26142"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5cf3ca4",
      "metadata": {
        "id": "e5cf3ca4"
      },
      "source": [
        "We create ``CustomDataset`` class that inherits from PyTorch’s ``Dataset`` class and\n",
        "implements the two methods: ``__getitem__`` and ``__len__``. \\\n",
        "``__getitem__`` fetches a data sample for a given index, ``__len__`` returns the size of the dataset.\\\n",
        "When accessed with ``dataset[idx]``, it reads the ``idx``-th example and its corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "8089f18c",
      "metadata": {
        "id": "8089f18c"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.x = inputs\n",
        "        self.y = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x[idx]\n",
        "        y = self.y[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "4dbe9629",
      "metadata": {
        "id": "4dbe9629"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(inputs, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d20de5f3",
      "metadata": {
        "id": "d20de5f3",
        "outputId": "192304d1-8c3f-4c60-dba2-35a1bb237edc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1.0053, -0.4016]), tensor(1.))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "af32af97",
      "metadata": {
        "id": "af32af97"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=1, drop_last=False, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "4a13523a",
      "metadata": {
        "id": "4a13523a",
        "outputId": "22eaa1e8-6e39-42e9-d4b5-5e113bb0c401",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0:\n",
            " x: tensor([[-0.1526,  0.8668]]), \n",
            " y: tensor([1.])\n",
            "batch 1:\n",
            " x: tensor([[-1.3665, -0.5493]]), \n",
            " y: tensor([1.])\n",
            "batch 2:\n",
            " x: tensor([[ 1.0053, -0.4016]]), \n",
            " y: tensor([1.])\n",
            "batch 3:\n",
            " x: tensor([[0.1485, 0.2838]]), \n",
            " y: tensor([0.])\n",
            "batch 4:\n",
            " x: tensor([[-0.0609, -1.0048]]), \n",
            " y: tensor([0.])\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(dataloader):\n",
        "    print(f\"batch {i}:\\n x: {batch[0]}, \\n y: {batch[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caf5557a",
      "metadata": {
        "id": "caf5557a"
      },
      "source": [
        "### Putting it all together: Gradient-based training loop for binary classification\n",
        "\n",
        "The gradient-based training algorithm iteratively updates each model parameter with the gradient of the loss function with respect to that parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d4347f90",
      "metadata": {
        "id": "d4347f90",
        "outputId": "00d18f96-c855-4623-92c4-02bbed60c2ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 0.9591237306594849\n",
            "loss: 1.3382587432861328\n",
            "loss: 0.5646849870681763\n",
            "loss: 0.5787708759307861\n",
            "loss: 0.5729712843894958\n"
          ]
        }
      ],
      "source": [
        "# each epoch defines a complete pass over the training data\n",
        "\n",
        "for batch in dataloader:\n",
        "\n",
        "    # step 0: get the data and the correct labels\n",
        "    features, labels = batch\n",
        "\n",
        "    # step 1: use the model for prediction (forward pass)\n",
        "    y_pred = model(features)\n",
        "\n",
        "    # step 2: measure the error by comparing the predictions to the expected outputs\n",
        "    error = loss_function(y_pred.squeeze(-1), labels) #reduce y_pred in 1D\n",
        "\n",
        "    # step 3: backprop the loss signal through the graph, notifying each parameter of its gradient\n",
        "    error.backward()\n",
        "\n",
        "    # step 4: update model parameters using the gradient\n",
        "    optimizer.step()\n",
        "\n",
        "    # step 5: clear the previously computed gradients stored inside the model\n",
        "    model.zero_grad()\n",
        "\n",
        "    # step 6: reporting\n",
        "    print(f'loss: {error.item()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d666b6",
      "metadata": {
        "id": "07d666b6"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}