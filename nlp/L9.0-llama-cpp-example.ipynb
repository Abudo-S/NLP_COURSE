{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ec6d4039",
      "metadata": {
        "id": "ec6d4039"
      },
      "source": [
        "##### Master Degree in Computer Science and Data Science for Economics\n",
        "\n",
        "# llama.cpp\n",
        "\n",
        "### Sergio Picascia\n",
        "\n",
        "The main goal of [llama.cpp](https://github.com/ggml-org/llama.cpp) is to enable LLM inference with minimal setup and state-of-the-art performance on a wide range of hardware - locally and in the cloud.\n",
        "\n",
        "[llama-cpp-python](https://github.com/abetlen/llama-cpp-python) is a simply Python bindings for llama.cpp. To install the package, run:\n",
        "```\n",
        "pip install llama-cpp-python\n",
        "```\n",
        "\n",
        "- To install with CUDA support, set the GGML_CUDA=on environment variable before installing:\n",
        "```\n",
        "CMAKE_ARGS=\"-DGGML_CUDA=on\" pip install llama-cpp-python\n",
        "```\n",
        "\n",
        "- To install with Metal (MPS), set the GGML_METAL=on environment variable before installing:\n",
        "```\n",
        "CMAKE_ARGS=\"-DGGML_METAL=on\" pip install llama-cpp-python\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "043477b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "043477b8",
        "outputId": "c2910a15-8931-408b-dc4b-120f519bb604"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.2.90\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.90-cu122/llama_cpp_python-0.2.90-cp312-cp312-linux_x86_64.whl (443.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.2.90) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.2.90) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.90)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.2.90) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.90) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.90\n"
          ]
        }
      ],
      "source": [
        "# Install llama-cpp-python with GPU support\n",
        "%pip install llama-cpp-python==0.2.90 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Features of Llama\n",
        "\n",
        "* Diverse Sizes: The Llama family includes models of various sizes, ranging from 7 billion to 70 billion parameters. This allows developers to choose a model that balances performance and computational cost for their specific use case.\n",
        "\n",
        "* Open-Source Nature: By releasing Llama as an open-source project, Meta has enabled a massive community of researchers and developers to build upon, fine-tune, and deploy the models for a wide range of applications.\n",
        "\n",
        "* Architectural Foundation: The Llama models are built on the Transformer architecture, which is the standard for modern LLMs. They use a number of optimizations to improve performance and efficiency, such as Grouped-Query Attention and SwiGLU activation functions.\n",
        "\n",
        "Purpose: Llama models are designed for a variety of tasks, including text generation, summarization, question answering, and coding assistance. Their open nature makes them a popular choice for building custom applications and performing research."
      ],
      "metadata": {
        "id": "5Qa76rmrIp3c"
      },
      "id": "5Qa76rmrIp3c"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7cab5331",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "7cab5331",
        "outputId": "06681add-79b8-4282-a498-88e322e43d4c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to load shared library '/usr/local/lib/python3.12/dist-packages/llama_cpp/lib/libllama.so': libcuda.so.1: cannot open shared object file: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_cpp/llama_cpp.py\u001b[0m in \u001b[0;36m_load_shared_library\u001b[0;34m(lib_base_name)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_lib_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcdll_args\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: libcuda.so.1: cannot open shared object file: No such file or directory",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-678364189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLlama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_cpp/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mllama_cpp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mllama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.2.90\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_cpp/llama_cpp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m# Load the library\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0m_lib\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_shared_library\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_lib_base_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/llama_cpp/llama_cpp.py\u001b[0m in \u001b[0;36m_load_shared_library\u001b[0;34m(lib_base_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_lib_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcdll_args\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Failed to load shared library '{_lib_path}': {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     raise FileNotFoundError(\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to load shared library '/usr/local/lib/python3.12/dist-packages/llama_cpp/lib/libllama.so': libcuda.so.1: cannot open shared object file: No such file or directory"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea725e2a",
      "metadata": {
        "id": "ea725e2a"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "llm = Llama.from_pretrained(repo_id=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\", # repository name\n",
        "                            filename=\"Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\", # model file\n",
        "                            n_gpu_layers=-1, # use all GPU layers\n",
        "                            n_ctx=32768, # context size\n",
        "                            flash_attn=True, # use flash attention\n",
        "                            chat_format=\"llama-3\", # chat format\n",
        "                            verbose=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3822cd1d",
      "metadata": {
        "id": "3822cd1d"
      },
      "outputs": [],
      "source": [
        "# Output generation\n",
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"What are the planets of the solar system?\",\n",
        "            },\n",
        "        ],\n",
        "        temperature=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b2036e",
      "metadata": {
        "id": "14b2036e",
        "outputId": "df6c2027-a165-4b6b-cfb3-f89aaab5ab4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-4febcd29-287f-45c1-be98-baa194c3f2f2',\n",
              " 'object': 'chat.completion',\n",
              " 'created': 1745305781,\n",
              " 'model': '/Users/sergiopicascia/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/bf5b95e96dac0462e2a09145ec66cae9a3f12067/./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf',\n",
              " 'choices': [{'index': 0,\n",
              "   'message': {'role': 'assistant',\n",
              "    'content': 'The planets of our solar system, in order from the Sun, are:\\n\\n1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune\\n\\nNote: Pluto was previously considered a planet, but in 2006 it was reclassified as a dwarf planet by the International Astronomical Union (IAU).\\n\\nHere\\'s a fun way to remember the order of the planets:\\n\\n\"Mary\\'s Violet Eyes Make Jeremy Stay Up Nights\"\\n\\nThe first letter of each word corresponds to the first letter of each planet\\'s name!\\n\\nWould you like to know more about any of the planets?'},\n",
              "   'logprobs': None,\n",
              "   'finish_reason': 'stop'}],\n",
              " 'usage': {'prompt_tokens': 30, 'completion_tokens': 134, 'total_tokens': 164}}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad190d75",
      "metadata": {
        "id": "ad190d75",
        "outputId": "178d8fa4-cb2e-42b0-c01a-5832edda03ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': 'The planets of our solar system, in order from the Sun, are:\\n\\n1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune\\n\\nNote: Pluto was previously considered a planet, but in 2006 it was reclassified as a dwarf planet by the International Astronomical Union (IAU).\\n\\nHere\\'s a fun way to remember the order of the planets:\\n\\n\"Mary\\'s Violet Eyes Make Jeremy Stay Up Nights\"\\n\\nThe first letter of each word corresponds to the first letter of each planet\\'s name!\\n\\nWould you like to know more about any of the planets?'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output[\"choices\"][0][\"message\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbcd80e8",
      "metadata": {
        "id": "dbcd80e8"
      },
      "outputs": [],
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Explain more easily the previous answer.\",\n",
        "            },\n",
        "        ],\n",
        "        temperature=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "448aef9e",
      "metadata": {
        "id": "448aef9e",
        "outputId": "eba89e4b-32f3-4710-be48-8ce7530e221e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': \"Since I didn't give an answer previously, let's start fresh.\\n\\nYou asked me to explain something, but I didn't receive a specific question. Could you please ask me something, and I'll do my best to provide a clear and easy-to-understand explanation?\"}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The model does not retain the context of the previous conversation, so we need to provide the context again.\n",
        "output[\"choices\"][0][\"message\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b7f7e3",
      "metadata": {
        "id": "e4b7f7e3"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"You are a helpful assistant.\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What are the planets of the solar system?\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96e7c0d9",
      "metadata": {
        "id": "96e7c0d9"
      },
      "outputs": [],
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea2799c0",
      "metadata": {
        "id": "ea2799c0",
        "outputId": "b375f6a8-f5d5-452a-84b0-bf59bbd65b1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': 'There are 8 planets in our solar system. Here they are in order from the Sun:\\n\\n1. Mercury\\n2. Venus\\n3. Earth\\n4. Mars\\n5. Jupiter\\n6. Saturn\\n7. Uranus\\n8. Neptune\\n\\nNote: Pluto was previously considered a planet, but in 2006 it was reclassified as a dwarf planet by the International Astronomical Union (IAU).\\n\\nWould you like to know more about any of these planets?'}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output[\"choices\"][0][\"message\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ab937f",
      "metadata": {
        "id": "f4ab937f"
      },
      "outputs": [],
      "source": [
        "messages.append(output[\"choices\"][0][\"message\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3668935",
      "metadata": {
        "id": "e3668935"
      },
      "outputs": [],
      "source": [
        "messages.append({\"role\": \"user\", \"content\": \"Order the planets in inverse order.\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d84fba95",
      "metadata": {
        "id": "d84fba95"
      },
      "outputs": [],
      "source": [
        "output = llm.create_chat_completion(\n",
        "        messages=messages,\n",
        "        temperature=0.7,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13560113",
      "metadata": {
        "id": "13560113",
        "outputId": "a38505eb-a742-4c6a-cbde-2f61f5a5b236"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': 'Here are the 8 planets in our solar system in reverse order from the Sun:\\n\\n1. Neptune\\n2. Uranus\\n3. Saturn\\n4. Jupiter\\n5. Mars\\n6. Earth\\n7. Venus\\n8. Mercury'}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output[\"choices\"][0][\"message\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf7494e",
      "metadata": {
        "id": "1bf7494e",
        "outputId": "d01c44a9-109a-448e-9f6a-3b6049963e20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'prompt_tokens': 144, 'completion_tokens': 49, 'total_tokens': 193}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number of tokens used\n",
        "output[\"usage\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3078d91",
      "metadata": {
        "id": "f3078d91",
        "outputId": "b2fe2f12-048b-4fff-b0cf-9560c1089918"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[128000, 3923, 527, 279, 33975, 315, 279, 13238, 1887, 30]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# How the model tokenizes the input\n",
        "llm.tokenize(\"What are the planets of the solar system?\".encode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "649cb4eb",
      "metadata": {
        "id": "649cb4eb",
        "outputId": "8d2483ae-ada8-4ced-ae40-e6ada858fd92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 8 planets in our solar system. Here's a list of them in order from the Sun:\n",
            "\n",
            "1. Mercury\n",
            "2. Venus\n",
            "3. Earth\n",
            "4. Mars\n",
            "5. Jupiter\n",
            "6. Saturn\n",
            "7. Uranus\n",
            "8. Neptune\n",
            "\n",
            "Note: Pluto was previously considered a planet, but in 2006, it was reclassified as a dwarf planet by the International Astronomical Union (IAU).\n",
            "\n",
            "Would you like to know more about a specific planet or the solar system in general?"
          ]
        }
      ],
      "source": [
        "# Example of streaming output\n",
        "for output in llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What are the planets of the solar system?\",\n",
        "        },\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    stream=True,\n",
        "):\n",
        "    print(output[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb5df4ce",
      "metadata": {
        "id": "bb5df4ce",
        "outputId": "6c75d599-f8db-482f-8efa-06d3b27784ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 8 planets in our solar system, which are:\n",
            "\n",
            "1. Mercury\n",
            "2. Venus\n",
            "3. Earth\n",
            "4. Mars\n",
            "5. Jupiter\n",
            "6. Saturn\n",
            "7. Uranus\n",
            "8. Neptune\n",
            "\n",
            "Note: Pluto was previously considered a planet, but in 2006 it was reclassified as a dwarf planet by the International Astronomical Union (IAU).\n",
            "\n",
            "Here's a fun way to remember the order of the planets:\n",
            "\n",
            "\"My Very Excellent Mother Just Served Us Nachos\"\n",
            "\n",
            "M - Mercury\n",
            "V - Venus\n",
            "E - Earth\n",
            "M - Mars\n",
            "J - Jupiter\n",
            "S - Saturn\n",
            "U - Uranus\n",
            "N - Neptune\n",
            "\n",
            "I hope that helps!"
          ]
        }
      ],
      "source": [
        "# Set the temperature to 0 for deterministic output\n",
        "for output in llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What are the planets of the solar system?\",\n",
        "        },\n",
        "    ],\n",
        "    temperature=0,\n",
        "    stream=True, #Enables streaming, allowing you to receive the output in chunks as it is generated.\n",
        "    #When stream=False (the default), you would have to wait until the model has finished generating the entire response before receiving anything.\n",
        "):\n",
        "    print(output[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2ead6b2",
      "metadata": {
        "id": "e2ead6b2",
        "outputId": "49e16f55-27e2-4ef6-be7a-f893b5ef5289"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"planets\": [\n",
            "    {\n",
            "      \"name\": \"Mercury\",\n",
            "      \"order\": 1\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Venus\",\n",
            "      \"order\": 2\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Earth\",\n",
            "      \"order\": 3\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Mars\",\n",
            "      \"order\": 4\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Jupiter\",\n",
            "      \"order\": 5\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Saturn\",\n",
            "      \"order\": 6\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Uranus\",\n",
            "      \"order\": 7\n",
            "    },\n",
            "    {\n",
            "      \"name\": \"Neptune\",\n",
            "      \"order\": 8\n",
            "    }\n",
            "  ]\n",
            "}"
          ]
        }
      ],
      "source": [
        "# Enforce the output to be in JSON format\n",
        "for output in llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant answering in JSON format.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What are the planets of the solar system?\",\n",
        "        },\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    stream=True,\n",
        "    response_format={\"type\": \"json_object\"}, #specify reponse format\n",
        "):\n",
        "    print(output[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3c5bc4b",
      "metadata": {
        "id": "d3c5bc4b",
        "outputId": "96c51854-5cd8-4a88-e428-9a15cf7615de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{\"planet\": \"Mercury\", \"distance_from_sun\": 57.9}, {\"planet\": \"Venus\", \"distance_from_sun\": 108.2}, {\"planet\": \"Earth\", \"distance_from_sun\": 149.6}, {\"planet\": \"Mars\", \"distance_from_sun\": 227.9}, {\"planet\": \"Jupiter\", \"distance_from_sun\": 778.3}, {\"planet\": \"Saturn\", \"distance_from_sun\": 1426.7}, {\"planet\": \"Uranus\", \"distance_from_sun\": 2870.9}, {\"planet\": \"Neptune\", \"distance_from_sun\": 4497.0}]"
          ]
        }
      ],
      "source": [
        "# Enforce the output to follow a specific JSON schema\n",
        "for output in llm.create_chat_completion(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are a helpful assistant answering in JSON format.\",\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"What are the planets of the solar system?\",\n",
        "        },\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    stream=True,\n",
        "    response_format={ #specify response schema\n",
        "        \"type\": \"json_object\",\n",
        "        \"schema\": {\n",
        "            \"type\": \"array\",\n",
        "            \"items\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"planet\": {\"type\": \"string\"},\n",
        "                    \"distance_from_sun\": {\"type\": \"number\"}\n",
        "                    },\n",
        "                \"required\": [\"planet\", \"distance_from_sun\"]\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "):\n",
        "    print(output[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522d64b8",
      "metadata": {
        "id": "522d64b8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "agraria",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}